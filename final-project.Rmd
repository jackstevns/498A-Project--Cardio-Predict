---
title: "Capstone Project"
author: "Jack Stevens"
date: "2024-04-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, include = FALSE)
```

```{r}
data <- read.csv("cardio_train.csv", sep = ';' )
```

## Cleaning data

```{r}
data <- subset(data, ap_hi <= 200 & ap_hi >= 60)
data <- subset(data, ap_lo <= 140 & ap_lo >= 40)
data <- subset(data, select = -weight)
(num_rows <- dim(data)[1])

```
Remaining Percentage of Data

```{r}
num_rows / 70000
```

```{r}
data <-  data[-1]

```

## Converting to Factors

```{r}
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)

```

Correlation Matrix

```{r}
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
```


```{R}
# Show the numeric predictors
cor(numeric_data)

```

Standardizing Values

```{r}
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
```

```{r}
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
```


### Number of Patients with and without Cardiovascular disease

```{r}
sum(data$cardio == 1) / 70000
sum(data$cardio == 0) / 70000
```

## Intial Logistic Model

```{r}
model <- glm(cardio ~., data = data, family = 'binomial')
summary(model)
```
### Significant Coeffiecents

```{r}
p_values <-  coef(summary(model))[, "Pr(>|z|)"]

significant_predictors <- p_values[p_values < 0.05]
(sig_predictors_names <- (names(significant_predictors[-1])))
```
### Reduced Size Model 

```{r}

model <- glm(cardio ~ age  + ap_hi + ap_lo + cholesterol + 
               gluc + smoke + alco + active, data = data, family = "binomial")

summary(model)


```

## Validation Split

```{r}
set.seed(1)
size_d <- dim(data)[1]
train_indices <- sample(size_d, size_d * .80)


```

```{r}
method <- c('logistic Regression', 'LDA', 'QDA', 'NB', 
            "Decision Tree", "KNN", "Random Forest")
```

```{r}
significant_predictors <-  c("cardio", "age", "ap_hi", "ap_lo", " cholesterol", 
              "gluc", "smoke",  "alco", "active")

selected <-  data[, which(names(data) %in% significant_predictors)]
```

```{r}
library(dplyr)
test_errors <-  numeric(7)
train_errors <-  numeric(7)
train.X <- selected[train_indices, -which(names(selected) == "cardio")]
train.Y <- selected[train_indices, "cardio"]
test.X <- selected[-train_indices, -which(names(selected) == "cardio")]
test.Y <- selected[-train_indices, "cardio"]
```



## Logistic Model 

```{r}

glm.fit = glm(cardio ~ ., data = selected,
            subset = train_indices, family = "binomial")


glm.probs = predict(glm.fit, test.X, type = "response")
glm.pred = rep(0, length(test.Y))
glm.pred[glm.probs > 0.5] = 1

(test_errors[1] = 1 - mean(glm.pred == test.Y))

```



```{R}

glm.probs = predict(glm.fit, train.X, type = "response")
glm.pred = rep(0, length(train.Y))
glm.pred[glm.probs > 0.5] = 1

(train_errors[1] = 1 - mean(glm.pred == train.Y))
``` 
 
## LDA 

```{r}  
library(MASS) 
  #LDA 
  lda.fit <-  lda(cardio ~ ., data = selected, 
                subset = train_indices)

  lda.pred <- predict(lda.fit , test.X)

  lda.class <- lda.pred$class

  (test_errors[2] = 1 - mean(lda.class == test.Y))

```


```{R}

  lda.pred <- predict(lda.fit , train.X)

  lda.class <- lda.pred$class


  (train_errors[2] = 1 - mean(lda.class == train.Y))
```

## QDA


```{r}  
  #QDA
  qda.fit <-  qda(cardio ~ . , data = selected,
                subset = train_indices)


  qda.pred <- predict(qda.fit , test.X)
  

  qda.class <- qda.pred$class


  (test_errors[3] = 1 - mean(qda.class == test.Y))

```

```{r}


  qda.pred <- predict(qda.fit , train.X)

  qda.class <- qda.pred$class


  (train_errors[3] = 1 - mean(qda.class == train.Y))
```

## Naive Bayes 

```{r}
library(e1071)
  # NB
   nb.fit <- naiveBayes(cardio ~ . ,
                      data = selected, subset = train_indices)
 
  nb.class = predict(nb.fit, test.X)

  (test_errors[4] = 1 - mean(nb.class == test.Y))

```

```{r}

  nb.class = predict(nb.fit, train.X)

  (train_errors[4] = 1 - mean(nb.class == train.Y))


```




## Decision Tree

```{r}
library(tree)
tree.cardio <- tree(cardio~., selected, subset = train_indices)

tree.pred <- predict(tree.cardio, test.X, type = "class")
table(tree.pred, test.Y)

(test_errors[5] <- 1 - mean(tree.pred ==  test.Y))

```

```{r}
tree.pred <- predict(tree.cardio, train.X, type = "class")
table(tree.pred, train.Y)

(train_errors[5] <- 1 - mean(tree.pred ==  train.Y))
```
```{r}
plot(tree.cardio)
text(tree.cardio, pretty = 0)

```
```{r}
## Takes a while to run
    library(class)
    k_test_error = numeric(6)
    k <- (1:6) *2 - 1

    min_test_error = 1
    k_value_test = 0

    for( i in 1:6){
        knn.pred <- knn(train.X, test.X, train.Y, k = 2*i - 1)
        test_error <- 1 - mean(test.Y == knn.pred)

        if(test_error < min_test_error){
          min_test_error <- test_error
          k_value_test <- 2*i - 1
        }
        k_test_error[i] <- test_error
      }


    plot(k,  k_test_error, xlab = "k",
         ylab = "test error", main = "Test error vs K")
    lines(k, k_test_error, col = 'red')
    points(k_value_test, min_test_error, col = "blue", pch = 19, cex = 2)

df <- data.frame(k,  k_test_error)

df

```



```{r}
knn.pred <- knn(train.X, test.X, train.Y, k = 7)
test_error <- 1 - mean(test.Y == knn.pred)
test_errors[6] <- test_error
table(knn.pred, test.Y)
```

```{r}
train_error <- 1 - mean(train.Y == knn.pred)
train_errors[6] <- train_error

```



```{r}
library(randomForest)
rf.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
      
                                              importance = TRUE)
```

```{r}
print(1 - rf.cardio$err.rate[rf.cardio$ntree, "OOB"])

```
``

```{R}
varImpPlot(rf.cardio)

```


```{r}
library(randomForest)
rf.cardio_reduced <- randomForest(cardio ~ ap_hi + age + ap_lo + gluc + active
                                  , data = selected, subset = train_indices,

                                              importance = TRUE)
```

```{r}
print(1 - rf.cardio_reduced$err.rate[rf.cardio_reduced$ntree, "OOB"])

```



```{r}
yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
```

```{r}
yhat.rf <- predict(rf.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
```



```{r}
accuracy_test <- 1 - test_errors
accuracy_train <- 1 - train_errors
df_interaction <- data.frame(method, test_errors, accuracy_test, train_errors, 
                             accuracy_train) 
df_sorted <- arrange(df_interaction, test_errors)
df_sorted
```
Logistic and Regression looks good. Maybe PCA / Lasso might 
help with logistic regression accuracy. This is a warming with the logistic 
regression model about converges? I don't know? 



#### testing new features in Random Forest model 
```{r}
names(data)

```

```{R}

library(randomForest)
rf.cardio_reduced <- randomForest(cardio ~ .
                                  , data = data, subset = train_indices,

                                              importance = TRUE)
```

```{r}

print(1 - rf.cardio_reduced$err.rate[rf.cardio_reduced$ntree, "OOB"])
yhat.rf <- predict(rf.cardio_reduced, newdata = data[-train_indices, ])
print( mean(yhat.rf == test.Y))
```




```{r}
varImpPlot(rf.cardio_reduced, main = "Random Forest Model-Full")

```
```{r}

library(ggplot2)
library(reshape2)

conf_matrix <- table( Actual = test.Y, Predicted = yhat.rf)


cm_melted <- melt(conf_matrix)


ggplot(data = cm_melted, aes(y = Actual, x = Predicted, fill = value)) +
    geom_tile() +
    geom_text(aes(label = value), vjust = 1) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = "Confusion Matrix", y = "Actual Class", x = "Predicted Class") +
    theme_minimal()

```


```{R}

library(randomForest)
rf.cardio_reduced <- randomForest(cardio ~ ap_hi + cholesterol + age + ap_lo + gluc + height
                                  , data = data, subset = train_indices,
                                  importance = TRUE)
```

```{r}
print(1 - rf.cardio_reduced$err.rate[rf.cardio_reduced$ntree, "OOB"])
yhat.rf <- predict(rf.cardio_reduced, newdata = data[-train_indices, ])
print( mean(yhat.rf == test.Y))
```

```{r}

library(ggplot2)
library(reshape2)

conf_matrix <- table( Actual = test.Y, Predicted = yhat.rf)

cm_melted <- melt(conf_matrix)


ggplot(data = cm_melted, aes(y = Actual, x = Predicted, fill = value)) +
    geom_tile() +
    geom_text(aes(label = value), vjust = 1) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = "Confusion Matrix", y = "Actual Class", x = "Predicted Class") +
    theme_minimal()

```



```{r}
test <- data[-train_indices, ]
predict.rf <- predict(rf.cardio_reduced, newdata = test, type = "prob")
library(pROC)
ROC_rf <- roc(test$cardio, predict.rf[,2])
```

```{R}
plot(ROC_rf, col = "red", main = "ROC For Random Forest")
```
```{R}
varImpPlot(rf.cardio_reduced, main = "Random Forest Model-Redcued")
```

```{r}

mtry_test <-  numeric(24)
tree_test <-  numeric(24)
accuracy <- numeric(24)
ntree <-  c(100, 300, 500, 1000)
mtrys <- c(1, 2, 3, 4, 5, 6)

```

### Hyperpamater Tunning

```{r}

init <-  1
for( i in 1:4){
  for(j in 1:6){
    rf.cardio <- randomForest(cardio ~ ap_hi + cholesterol + age + ap_lo + gluc,
                 data = data, subset = train_indices, mtry = mtrys[j],
                 ntree = ntree[i], importance = TRUE)
    mtry_test[init] <- mtrys[j]
    tree_test[init] <- ntree[i]
    yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
    accuracy[init] = mean(yhat.rf == test.Y)
    init <- init + 1
   
  }
}
```

```{R}
df <- arrange(data.frame(mtry_test, tree_test,accuracy), accuracy)
df

```

```{r}
tail(arrange(df, accuracy))
```


```{r}
random_forest_tuned <-  randomForest(cardio ~ ap_hi + cholesterol + age + ap_lo + gluc 
                                  , data = data, subset = train_indices, mtry = 2, ntree = 300,
                                  importance = TRUE)
print(1 - random_forest_tuned$err.rate[300, "OOB"])
  yhat.rf <- predict(random_forest_tuned, newdata = data[-train_indices, ])
 mean(yhat.rf == test.Y)

```

