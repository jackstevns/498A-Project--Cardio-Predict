bag.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
mtry = 8, importance = TRUE)
yhat.rf <- predict(bag.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
yhat.rf <- predict(bag.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
library(randomForest)
rf.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
importance = TRUE)
yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
test_errrors[7] = 1 - mean(yhat.rf == test.Y)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
names(data)
data <-  data[-1]
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
# If you want to exclude the outcome variable, make sure it's not numeric or handle it separately
if("outcome" %in% names(numeric_data)) {
numeric_data <- numeric_data[, !names(numeric_data) %in% "outcome"]
}
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
attach(data)
plot(cardio, age)
plot(cardio, height)
sum(data$cardio == 1)
sum(data$cardio == 0)
model <- glm(cardio ~., data = data, family = 'binomial', control = glm.control(maxit = 100))
summary(model)
p_values <-  coef(summary(model))[, "Pr(>|z|)"]
significant_predictors <- p_values[p_values < 0.05]
(sig_predictors_names <- (names(significant_predictors[-1])))
model <- glm(cardio ~ age + height + weight + ap_hi + ap_lo + cholesterol +
gluc + smoke + alco + active, data = data, family = "binomial")
summary(model)
set.seed(1)
size_d <- dim(data)[1]
train_indices <- sample(size_d, size_d * .80)
method <- c('logistic Regression', 'LDA', 'QDA', 'NB',
"Decision Tree", "KNN", "Random Forest", 'boosting')
library(dplyr)
test_errors <-  numeric(8)
train_errors <-  numeric(8)
selected <-  data
train.X <- selected[train_indices, -which(names(selected) == "cardio")]
train.Y <- selected[train_indices, "cardio"]
test.X <- selected[-train_indices, -which(names(selected) == "cardio")]
test.Y <- selected[-train_indices, "cardio"]
#logistic Classification
glm.fit = glm(cardio ~ ., data = selected,
subset = train_indices, family = "binomial")
#compute the estimated class probability
glm.probs = predict(glm.fit, test.X, type = "response")
glm.pred = rep(0, length(test.Y))
glm.pred[glm.probs > 0.5] = 1
(test_errors[1] = 1 - mean(glm.pred == test.Y))
#compute the estimated class probability
glm.probs = predict(glm.fit, train.X, type = "response")
glm.pred = rep(0, length(train.Y))
glm.pred[glm.probs > 0.5] = 1
(train_errors[1] = 1 - mean(glm.pred == train.Y))
library(MASS)
#LDA
lda.fit <-  lda(cardio ~ ., data = selected,
subset = train_indices)
# make predictions
lda.pred <- predict(lda.fit , test.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(test_errors[2] = 1 - mean(lda.class == test.Y))
# make predictions
lda.pred <- predict(lda.fit , train.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(train_errors[2] = 1 - mean(lda.class == train.Y))
#QDA
qda.fit <-  qda(cardio ~ . , data = selected,
subset = train_indices)
# make predictions
qda.pred <- predict(qda.fit , test.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(test_errors[3] = 1 - mean(qda.class == test.Y))
# make predictions
qda.pred <- predict(qda.fit , train.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(train_errors[3] = 1 - mean(qda.class == train.Y))
library(e1071)
# NB
nb.fit <- naiveBayes(cardio ~ . ,
data = selected, subset = train_indices)
# predict class labels for the test data
nb.class = predict(nb.fit, test.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(test_errors[4] = 1 - mean(nb.class == test.Y))
# predict class labels for the test data
nb.class = predict(nb.fit, train.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(train_errors[4] = 1 - mean(nb.class == train.Y))
library(tree)
tree.cardio <- tree(cardio~., selected, subset = train_indices)
tree.pred <- predict(tree.cardio, test.X, type = "class")
table(tree.pred, test.Y)
(test_errors[5] <- 1 - mean(tree.pred ==  test.Y))
tree.pred <- predict(tree.cardio, train.X, type = "class")
table(tree.pred, train.Y)
(train_errors[5] <- 1 - mean(tree.pred ==  train.Y))
### Takes a while to run
library(class)
k_test_error = numeric(6)
k <- (1:6) *2 - 1
min_test_error = 1
k_value_test = 0
for( i in 1:6){
knn.pred <- knn(train.X, test.X, train.Y, k = 2*i - 1)
test_error <- 1 - mean(test.Y == knn.pred)
if(test_error < min_test_error){
min_test_error <- test_error
k_value_test <- 2*i - 1
}
k_test_error[i] <- test_error
}
plot(k,  k_test_error, xlab = "k",
ylab = "test error", main = "Test error vs K")
lines(k, k_test_error, col = 'red')
points(k_value_test, min_test_error, col = "blue", pch = 19, cex = 2)
df <- data.frame(k,  k_test_error)
df
knn.pred <- knn(train.X, test.X, train.Y, k = 7)
test_error <- 1 - mean(test.Y == knn.pred)
test_errors[6] <- test_error
train_error <- 1 - mean(train.Y == knn.pred)
train_errors[6] <- train_error
library(randomForest)
bag.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
mtry = 8, importance = TRUE)
yhat.rf <- predict(bag.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
yhat.rf <- predict(bag.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
library(randomForest)
rf.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
importance = TRUE)
yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
yhat.rf <- predict(rf.cardio, newdata = data[train_indices, ])
train_errrors[7] = 1 - mean(yhat.rf == train.Y)
yhat.rf <- predict(rf.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
library(gbm)
boost.carido <- gbm(cardio ~., data = data[train_indices,],
distribution = 'guassian', n.tree = 5000,
interactions.depth = 4)
install.packages("gbm3")
library(gbm3)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
dim(data)[1] - (length(data$ap_hi > 200) + length =(data$ap_hi < 60))
dim(data)[1] - (length(data$ap_hi > 200) + length(data$ap_hi < 60))
```{r}
dim(data)[1] - (length(data$ap_hi > 200) + length(data$ap_hi < 60))
dim(data)[1] - (sum(data$ap_hi > 200) + sum(data$ap_hi < 60))
69712/7000
69712/70000
(sum(data$ap_hi > 200) + sum(data$ap_hi < 60))
hist(data$height)
hist(data$height > 140)
hist(data[data$height > 140]$height)
filtered_data <- data[data$height >= 140 || data$height <= 180]
hist(filtered_data$height)
dim(filtered_data)
filtered_data <- data[data$height >= 140 && data$height <= 180]
dim(filtered_data)
attach(data)
sum(height < 140)
sum(height < 180)
hist(height)
lenght(height > 190)
length(height > 190)
attach(data)
length(height < 190)
hist(height)
sum(data$height < 190)
hist(ap_hi)
attach(data)
data <- data[data["ap_hi"] < 200]
hist(ap_hi)
data <- data[data["ap_hi"] < 200]
hist(data$ap_hi)
hist(data["ap_hi"])
data <- data[data$ap_hi < 200]
sum(data$ap_hi < 200)
sum(data$ap_hi < 200)
detach(data)
detach(data)
sum(data$ap_hi < 200)
sum(data['ap_hi'] < 200)
detach(data)
data = data[data['ap_hi'] < 200]
hist(data$ap_hi)
summary(data)
detach(data)
detach(data)
data = subset(data, ap_hi <= 200)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
data = subset(data, ap_hi <= 200)
hist(data$ap_hi)
dim(data)[1]
data = subset(data, ap_hi <= 200 && ap_hi >= 60)
data = subset(data, ap_lo <= 90 && ap_hi >= 30)
data = subset(data, height  <= 140 && ap_hi >= 190)
data = subset(data, select = -weight)
dim(data)[1]
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
data = subset(data, ap_hi <= 200 && ap_hi >= 60)
data = subset(data, ap_lo <= 90 && ap_hi >= 30)
data = subset(data, height  <= 190 && ap_hi >= 140)
data = subset(data, select = -weight)
dim(data)[1]
data = subset(data, ap_hi <= 200 && ap_hi >= 60)
data = subset(data, ap_lo <= 90 && ap_hi >= 30)
data = subset(data, height  <= 190 && ap_hi >= 140)
#data = subset(data, select = -weight)
dim(data)[1]
# Get the number of rows in the filtered dataset
(num_rows <- dim(data)[1])
# Filter data based on conditions for 'ap_hi' (Systolic Blood Pressure)
data <- subset(data, ap_hi <= 200 & ap_hi >= 60)
# Filter data based on conditions for 'ap_lo' (Diastolic Blood Pressure)
data <- subset(data, ap_lo <= 90 & ap_lo >= 30)
# Filter data based on conditions for 'height'
data <- subset(data, height <= 190 & height >= 140)
# Remove the 'weight' column
data <- subset(data, select = -weight)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
# Filter data based on conditions for 'ap_hi' (Systolic Blood Pressure)
data <- subset(data, ap_hi <= 200 & ap_hi >= 60)
# Filter data based on conditions for 'ap_lo' (Diastolic Blood Pressure)
data <- subset(data, ap_lo <= 90 & ap_lo >= 30)
# Filter data based on conditions for 'height'
data <- subset(data, height <= 190 & height >= 140)
# Remove the 'weight' column
data <- subset(data, select = -weight)
# Get the number of rows in the filtered dataset
(num_rows <- dim(data)[1])
num_rows / 70000
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
# Filter data based on conditions for 'ap_hi' (Systolic Blood Pressure)
data <- subset(data, ap_hi <= 200 & ap_hi >= 60)
# Filter data based on conditions for 'ap_lo' (Diastolic Blood Pressure)
data <- subset(data, ap_lo <= 90 & ap_lo >= 30)
# Filter data based on conditions for 'height'
data <- subset(data, height <= 190 & height >= 140)
# Remove the 'weight' column
data <- subset(data, select = -weight)
# Get the number of rows in the filtered dataset
(num_rows <- dim(data)[1])
num_rows / 70000
summary(data)
names(data)
data <-  data[-1]
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
numeric_data <- numeric_data[, -which(names(numeric_data) == "weight")]
names(numeric_data)
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
attach(data)
plot(cardio, age)
head(data)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
# Filter data based on conditions for 'ap_hi' (Systolic Blood Pressure)
data <- subset(data, ap_hi <= 200 & ap_hi >= 60)
# Filter data based on conditions for 'ap_lo' (Diastolic Blood Pressure)
data <- subset(data, ap_lo <= 90 & ap_lo >= 30)
# Filter data based on conditions for 'height'
data <- subset(data, height <= 190 & height >= 140)
# Remove the 'weight' column
data <- subset(data, select = -weight)
# Get the number of rows in the filtered dataset
(num_rows <- dim(data)[1])
num_rows / 70000
summary(data)
head(data)
names(data)
data <-  data[-1]
names(data)
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
names(numeric_data)
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
summary(data)
# Filter data based on conditions for 'ap_hi' (Systolic Blood Pressure)
data <- subset(data, ap_hi <= 200 & ap_hi >= 60)
# Filter data based on conditions for 'ap_lo' (Diastolic Blood Pressure)
data <- subset(data, ap_lo <= 90 & ap_lo >= 30)
# Filter data based on conditions for 'height'
data <- subset(data, height <= 190 & height >= 140)
# Remove the 'weight' column
data <- subset(data, select = -weight)
# Get the number of rows in the filtered dataset
(num_rows <- dim(data)[1])
num_rows / 70000
summary(data)
head(data)
names(data)
data <-  data[-1]
names(data)
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
names(numeric_data)
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
sum(data$cardio == 1)
sum(data$cardio == 0)
model <- glm(cardio ~., data = data, family = 'binomial', control = glm.control(maxit = 100))
summary(model)
p_values <-  coef(summary(model))[, "Pr(>|z|)"]
significant_predictors <- p_values[p_values < 0.05]
(sig_predictors_names <- (names(significant_predictors[-1])))
model <- glm(cardio ~ age + height + weight + ap_hi + ap_lo + cholesterol +
gluc + smoke + alco + active, data = data, family = "binomial")
model <- glm(cardio ~ age + height + ap_hi + ap_lo + cholesterol +
gluc + smoke + alco + active, data = data, family = "binomial")
summary(model)
set.seed(1)
size_d <- dim(data)[1]
train_indices <- sample(size_d, size_d * .80)
method <- c('logistic Regression', 'LDA', 'QDA', 'NB',
"Decision Tree", "KNN", "Random Forest", 'boosting')
library(dplyr)
test_errors <-  numeric(8)
train_errors <-  numeric(8)
selected <-  data
train.X <- selected[train_indices, -which(names(selected) == "cardio")]
train.Y <- selected[train_indices, "cardio"]
test.X <- selected[-train_indices, -which(names(selected) == "cardio")]
test.Y <- selected[-train_indices, "cardio"]
head(selected)
#logistic Classification
glm.fit = glm(cardio ~ ., data = selected,
subset = train_indices, family = "binomial")
#compute the estimated class probability
glm.probs = predict(glm.fit, test.X, type = "response")
glm.pred = rep(0, length(test.Y))
glm.pred[glm.probs > 0.5] = 1
(test_errors[1] = 1 - mean(glm.pred == test.Y))
#compute the estimated class probability
glm.probs = predict(glm.fit, train.X, type = "response")
glm.pred = rep(0, length(train.Y))
glm.pred[glm.probs > 0.5] = 1
(train_errors[1] = 1 - mean(glm.pred == train.Y))
library(MASS)
#LDA
lda.fit <-  lda(cardio ~ ., data = selected,
subset = train_indices)
# make predictions
lda.pred <- predict(lda.fit , test.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(test_errors[2] = 1 - mean(lda.class == test.Y))
# make predictions
lda.pred <- predict(lda.fit , train.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(train_errors[2] = 1 - mean(lda.class == train.Y))
#QDA
qda.fit <-  qda(cardio ~ . , data = selected,
subset = train_indices)
# make predictions
qda.pred <- predict(qda.fit , test.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(test_errors[3] = 1 - mean(qda.class == test.Y))
# make predictions
qda.pred <- predict(qda.fit , train.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(train_errors[3] = 1 - mean(qda.class == train.Y))
library(e1071)
# NB
nb.fit <- naiveBayes(cardio ~ . ,
data = selected, subset = train_indices)
# predict class labels for the test data
nb.class = predict(nb.fit, test.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(test_errors[4] = 1 - mean(nb.class == test.Y))
# predict class labels for the test data
nb.class = predict(nb.fit, train.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(train_errors[4] = 1 - mean(nb.class == train.Y))
library(tree)
tree.cardio <- tree(cardio~., selected, subset = train_indices)
tree.pred <- predict(tree.cardio, test.X, type = "class")
table(tree.pred, test.Y)
(test_errors[5] <- 1 - mean(tree.pred ==  test.Y))
tree.pred <- predict(tree.cardio, train.X, type = "class")
table(tree.pred, train.Y)
(train_errors[5] <- 1 - mean(tree.pred ==  train.Y))
### Takes a while to run
#     library(class)
#     k_test_error = numeric(6)
#     k <- (1:6) *2 - 1
#
#     min_test_error = 1
#     k_value_test = 0
#
#     for( i in 1:6){
#         knn.pred <- knn(train.X, test.X, train.Y, k = 2*i - 1)
#         test_error <- 1 - mean(test.Y == knn.pred)
#
#         if(test_error < min_test_error){
#           min_test_error <- test_error
#           k_value_test <- 2*i - 1
#         }
#         k_test_error[i] <- test_error
#       }
#
#
#     plot(k,  k_test_error, xlab = "k",
#          ylab = "test error", main = "Test error vs K")
#     lines(k, k_test_error, col = 'red')
#     points(k_value_test, min_test_error, col = "blue", pch = 19, cex = 2)
#
# df <- data.frame(k,  k_test_error)
#
# df
knn.pred <- knn(train.X, test.X, train.Y, k = 11)
