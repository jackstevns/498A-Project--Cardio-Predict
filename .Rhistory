install.packages('gbm')
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
names(data)
data <-  data[-1]
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
# If you want to exclude the outcome variable, make sure it's not numeric or handle it separately
if("outcome" %in% names(numeric_data)) {
numeric_data <- numeric_data[, !names(numeric_data) %in% "outcome"]
}
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
attach(data)
plot(cardio, age)
plot(cardio, height)
sum(data$cardio == 1)
sum(data$cardio == 0)
model <- glm(cardio ~., data = data, family = 'binomial', control = glm.control(maxit = 100))
summary(model)
p_values <-  coef(summary(model))[, "Pr(>|z|)"]
significant_predictors <- p_values[p_values < 0.05]
(sig_predictors_names <- (names(significant_predictors[-1])))
model <- glm(cardio ~ age + height + weight + ap_hi + ap_lo + cholesterol +
gluc + smoke + alco + active, data = data, family = "binomial")
summary(model)
set.seed(1)
size_d <- dim(data)[1]
train_indices <- sample(size_d, size_d * .80)
method <- c('logistic Regression', 'LDA', 'QDA', 'NB',
"Decision Tree", "KNN", "Random Forest ")
library(dplyr)
test_errors <-  numeric(7)
selected <-  data
train.X <- selected[train_indices, -which(names(selected) == "cardio")]
train.Y <- selected[train_indices, "cardio"]
test.X <- selected[-train_indices, -which(names(selected) == "cardio")]
test.Y <- selected[-train_indices, "cardio"]
#logistic Classification
glm.fit = glm(cardio ~ ., data = selected,
subset = train_indices, family = "binomial")
#compute the estimated class probability
glm.probs = predict(glm.fit, test.X, type = "response")
glm.pred = rep(0, length(test.Y))
glm.pred[glm.probs > 0.5] = 1
(test_errors[1] = 1 - mean(glm.pred == test.Y))
library(MASS)
#LDA
lda.fit <-  lda(cardio ~ ., data = selected,
subset = train_indices)
# make predictions
lda.pred <- predict(lda.fit , test.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(test_errors[2] = 1 - mean(lda.class == test.Y))
#QDA
qda.fit <-  qda(cardio ~ . , data = selected,
subset = train_indices)
# make predictions
qda.pred <- predict(qda.fit , test.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(test_errors[3] = 1 - mean(qda.class == test.Y))
library(e1071)
# NB
nb.fit <- naiveBayes(cardio ~ . ,
data = selected, subset = train_indices)
# predict class labels for the test data
nb.class = predict(nb.fit, test.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(test_errors[4] = 1 - mean(nb.class == test.Y))
library(tree)
tree.cardio <- tree(cardio~., selected, subset = train_indices)
tree.pred <- predict(tree.cardio, test.X, type = "class")
table(tree.pred, test.Y)
(test_errors[5] <- 1 - mean(tree.pred ==  test.Y))
### Takes a while to run
library(class)
k_test_error = numeric(6)
k <- (1:6) *2 - 1
min_test_error = 1
k_value_test = 0
for( i in 1:6){
knn.pred <- knn(train.X, test.X, train.Y, k = 2*i - 1)
test_error <- 1 - mean(test.Y == knn.pred)
if(test_error < min_test_error){
min_test_error <- test_error
k_value_test <- 2*i - 1
}
k_test_error[i] <- test_error
}
plot(k,  k_test_error, xlab = "k",
ylab = "test error", main = "Test error vs K")
lines(k, k_test_error, col = 'red')
points(k_value_test, min_test_error, col = "blue", pch = 19, cex = 2)
df <- data.frame(k,  k_test_error)
df
knn.pred <- knn(train.X, test.X, train.Y, k = 7)
test_error <- 1 - mean(test.Y == knn.pred)
test_errors[6] <- test_error
library(randomForest)
bag.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
mtry = 8, importance = TRUE)
yhat.rf <- predict(bag.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
library(randomForest)
rf.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices, importance = TRUE)
yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
test_errrors[7] = 1 - mean(yhat.rf == test.Y)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
names(data)
data <-  data[-1]
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
# If you want to exclude the outcome variable, make sure it's not numeric or handle it separately
if("outcome" %in% names(numeric_data)) {
numeric_data <- numeric_data[, !names(numeric_data) %in% "outcome"]
}
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
attach(data)
plot(cardio, age)
plot(cardio, height)
sum(data$cardio == 1)
sum(data$cardio == 0)
model <- glm(cardio ~., data = data, family = 'binomial', control = glm.control(maxit = 100))
summary(model)
p_values <-  coef(summary(model))[, "Pr(>|z|)"]
significant_predictors <- p_values[p_values < 0.05]
(sig_predictors_names <- (names(significant_predictors[-1])))
model <- glm(cardio ~ age + height + weight + ap_hi + ap_lo + cholesterol +
gluc + smoke + alco + active, data = data, family = "binomial")
summary(model)
set.seed(1)
size_d <- dim(data)[1]
train_indices <- sample(size_d, size_d * .80)
method <- c('logistic Regression', 'LDA', 'QDA', 'NB',
"Decision Tree", "KNN", "Random Forest", 'boosting')
library(dplyr)
test_errors <-  numeric(8)
train_errors <-  numeric(8)
selected <-  data
train.X <- selected[train_indices, -which(names(selected) == "cardio")]
train.Y <- selected[train_indices, "cardio"]
test.X <- selected[-train_indices, -which(names(selected) == "cardio")]
test.Y <- selected[-train_indices, "cardio"]
#logistic Classification
glm.fit = glm(cardio ~ ., data = selected,
subset = train_indices, family = "binomial")
#compute the estimated class probability
glm.probs = predict(glm.fit, test.X, type = "response")
glm.pred = rep(0, length(test.Y))
glm.pred[glm.probs > 0.5] = 1
(test_errors[1] = 1 - mean(glm.pred == test.Y))
#compute the estimated class probability
glm.probs = predict(glm.fit, train.X, type = "response")
glm.pred = rep(0, length(train.Y))
glm.pred[glm.probs > 0.5] = 1
(train_errors[1] = 1 - mean(glm.pred == train.Y))
library(MASS)
#LDA
lda.fit <-  lda(cardio ~ ., data = selected,
subset = train_indices)
# make predictions
lda.pred <- predict(lda.fit , test.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(test_errors[2] = 1 - mean(lda.class == test.Y))
# make predictions
lda.pred <- predict(lda.fit , train.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(train_errors[2] = 1 - mean(lda.class == train.Y))
#QDA
qda.fit <-  qda(cardio ~ . , data = selected,
subset = train_indices)
# make predictions
qda.pred <- predict(qda.fit , test.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(test_errors[3] = 1 - mean(qda.class == test.Y))
# make predictions
qda.pred <- predict(qda.fit , train.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(train_errors[3] = 1 - mean(qda.class == train.Y))
library(e1071)
# NB
nb.fit <- naiveBayes(cardio ~ . ,
data = selected, subset = train_indices)
# predict class labels for the test data
nb.class = predict(nb.fit, test.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(test_errors[4] = 1 - mean(nb.class == test.Y))
# predict class labels for the test data
nb.class = predict(nb.fit, train.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(train_errors[4] = 1 - mean(nb.class == train.Y))
library(tree)
tree.cardio <- tree(cardio~., selected, subset = train_indices)
tree.pred <- predict(tree.cardio, test.X, type = "class")
table(tree.pred, test.Y)
(test_errors[5] <- 1 - mean(tree.pred ==  test.Y))
tree.pred <- predict(tree.cardio, train.X, type = "class")
table(tree.pred, train.Y)
(train_errors[5] <- 1 - mean(tree.pred ==  train.Y))
### Takes a while to run
library(class)
k_test_error = numeric(6)
k <- (1:6) *2 - 1
min_test_error = 1
k_value_test = 0
for( i in 1:6){
knn.pred <- knn(train.X, test.X, train.Y, k = 2*i - 1)
test_error <- 1 - mean(test.Y == knn.pred)
if(test_error < min_test_error){
min_test_error <- test_error
k_value_test <- 2*i - 1
}
k_test_error[i] <- test_error
}
plot(k,  k_test_error, xlab = "k",
ylab = "test error", main = "Test error vs K")
lines(k, k_test_error, col = 'red')
points(k_value_test, min_test_error, col = "blue", pch = 19, cex = 2)
df <- data.frame(k,  k_test_error)
df
knn.pred <- knn(train.X, test.X, train.Y, k = 7)
test_error <- 1 - mean(test.Y == knn.pred)
test_errors[6] <- test_error
train_error <- 1 - mean(train.Y == knn.pred)
train_errors[6] <- train_error
library(randomForest)
bag.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
mtry = 8, importance = TRUE)
yhat.rf <- predict(bag.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
yhat.rf <- predict(bag.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
library(randomForest)
rf.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
importance = TRUE)
yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
test_errrors[7] = 1 - mean(yhat.rf == test.Y)
data <- read.csv("cardio_train.csv", sep = ';' )
head(data)
names(data)
data <-  data[-1]
data['gender'] <- as.factor(data$gender)
data['cholesterol'] <-  as.factor(data$cholesterol)
data['gluc'] <-  as.factor(data$gluc)
data['smoke'] <-  as.factor(data$smoke)
data['alco'] <- as.factor(data$alco)
data['active'] <- as.factor(data$active)
data['cardio'] <-  as.factor(data$cardio)
# Select only numeric predictors
numeric_data <- data[sapply(data, is.numeric)]
# If you want to exclude the outcome variable, make sure it's not numeric or handle it separately
if("outcome" %in% names(numeric_data)) {
numeric_data <- numeric_data[, !names(numeric_data) %in% "outcome"]
}
# Show the numeric predictors
cor(numeric_data)
numeric_data_scaled <- scale(numeric_data)
head(numeric_data_scaled)
non_numeric_data <- data[sapply(data, is.factor)]
data <- data.frame(numeric_data_scaled, non_numeric_data)
head(data)
names(data)
summary(data)
attach(data)
plot(cardio, age)
plot(cardio, height)
sum(data$cardio == 1)
sum(data$cardio == 0)
model <- glm(cardio ~., data = data, family = 'binomial', control = glm.control(maxit = 100))
summary(model)
p_values <-  coef(summary(model))[, "Pr(>|z|)"]
significant_predictors <- p_values[p_values < 0.05]
(sig_predictors_names <- (names(significant_predictors[-1])))
model <- glm(cardio ~ age + height + weight + ap_hi + ap_lo + cholesterol +
gluc + smoke + alco + active, data = data, family = "binomial")
summary(model)
set.seed(1)
size_d <- dim(data)[1]
train_indices <- sample(size_d, size_d * .80)
method <- c('logistic Regression', 'LDA', 'QDA', 'NB',
"Decision Tree", "KNN", "Random Forest", 'boosting')
library(dplyr)
test_errors <-  numeric(8)
train_errors <-  numeric(8)
selected <-  data
train.X <- selected[train_indices, -which(names(selected) == "cardio")]
train.Y <- selected[train_indices, "cardio"]
test.X <- selected[-train_indices, -which(names(selected) == "cardio")]
test.Y <- selected[-train_indices, "cardio"]
#logistic Classification
glm.fit = glm(cardio ~ ., data = selected,
subset = train_indices, family = "binomial")
#compute the estimated class probability
glm.probs = predict(glm.fit, test.X, type = "response")
glm.pred = rep(0, length(test.Y))
glm.pred[glm.probs > 0.5] = 1
(test_errors[1] = 1 - mean(glm.pred == test.Y))
#compute the estimated class probability
glm.probs = predict(glm.fit, train.X, type = "response")
glm.pred = rep(0, length(train.Y))
glm.pred[glm.probs > 0.5] = 1
(train_errors[1] = 1 - mean(glm.pred == train.Y))
library(MASS)
#LDA
lda.fit <-  lda(cardio ~ ., data = selected,
subset = train_indices)
# make predictions
lda.pred <- predict(lda.fit , test.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(test_errors[2] = 1 - mean(lda.class == test.Y))
# make predictions
lda.pred <- predict(lda.fit , train.X)
# produce a confusion matrix
lda.class <- lda.pred$class
# calculate accuracy on the test set
(train_errors[2] = 1 - mean(lda.class == train.Y))
#QDA
qda.fit <-  qda(cardio ~ . , data = selected,
subset = train_indices)
# make predictions
qda.pred <- predict(qda.fit , test.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(test_errors[3] = 1 - mean(qda.class == test.Y))
# make predictions
qda.pred <- predict(qda.fit , train.X)
# produce a confusion matrix
qda.class <- qda.pred$class
# calculate accuracy on the test set
(train_errors[3] = 1 - mean(qda.class == train.Y))
library(e1071)
# NB
nb.fit <- naiveBayes(cardio ~ . ,
data = selected, subset = train_indices)
# predict class labels for the test data
nb.class = predict(nb.fit, test.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(test_errors[4] = 1 - mean(nb.class == test.Y))
# predict class labels for the test data
nb.class = predict(nb.fit, train.X)
# print the confusion matrix for the test data
# compute the prediction accuracy on the test data
(train_errors[4] = 1 - mean(nb.class == train.Y))
library(tree)
tree.cardio <- tree(cardio~., selected, subset = train_indices)
tree.pred <- predict(tree.cardio, test.X, type = "class")
table(tree.pred, test.Y)
(test_errors[5] <- 1 - mean(tree.pred ==  test.Y))
tree.pred <- predict(tree.cardio, train.X, type = "class")
table(tree.pred, train.Y)
(train_errors[5] <- 1 - mean(tree.pred ==  train.Y))
### Takes a while to run
library(class)
k_test_error = numeric(6)
k <- (1:6) *2 - 1
min_test_error = 1
k_value_test = 0
for( i in 1:6){
knn.pred <- knn(train.X, test.X, train.Y, k = 2*i - 1)
test_error <- 1 - mean(test.Y == knn.pred)
if(test_error < min_test_error){
min_test_error <- test_error
k_value_test <- 2*i - 1
}
k_test_error[i] <- test_error
}
plot(k,  k_test_error, xlab = "k",
ylab = "test error", main = "Test error vs K")
lines(k, k_test_error, col = 'red')
points(k_value_test, min_test_error, col = "blue", pch = 19, cex = 2)
df <- data.frame(k,  k_test_error)
df
knn.pred <- knn(train.X, test.X, train.Y, k = 7)
test_error <- 1 - mean(test.Y == knn.pred)
test_errors[6] <- test_error
train_error <- 1 - mean(train.Y == knn.pred)
train_errors[6] <- train_error
library(randomForest)
bag.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
mtry = 8, importance = TRUE)
yhat.rf <- predict(bag.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
yhat.rf <- predict(bag.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
library(randomForest)
rf.cardio <- randomForest(cardio ~. , data = selected, subset = train_indices,
importance = TRUE)
yhat.rf <- predict(rf.cardio, newdata = data[-train_indices, ])
test_errors[7] = 1 - mean(yhat.rf == test.Y)
yhat.rf <- predict(rf.cardio, newdata = data[train_indices, ])
train_errrors[7] = 1 - mean(yhat.rf == train.Y)
yhat.rf <- predict(rf.cardio, newdata = data[train_indices, ])
train_errors[7] = 1 - mean(yhat.rf == train.Y)
library(gbm)
boost.carido <- gbm(cardio ~., data = data[train_indices,],
distribution = 'guassian', n.tree = 5000,
interactions.depth = 4)
install.packages("gbm3")
library(gbm3)
